{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a18998-d15c-48c4-b864-9797924e3a99",
     "showTitle": true,
     "title": "Input Parameter Requirements"
    }
   },
   "outputs": [],
   "source": [
    "# Source SFTP Access Name: Required\n",
    "# Source SFTP File Path: Required [Formats allowed: /filename.txt, /filename_yyyymmdd.txt, /filename_yyyymmddHHMMSS.txt]\n",
    "# Target S3 Access Name: Required\n",
    "# Target S3 File Path: Required [Formats allowed: s3://bucket/prefix/filename.txt, s3://bucket/prefix/filename_yyyymmdd.txt, _yyyymmddHHMMSS.txt]\n",
    "# Email Notification: Defaults to skip [Requires email id to send notification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9f323b-c505-4e24-b487-224f45f6ea60",
     "showTitle": true,
     "title": "Creating widgets"
    }
   },
   "outputs": [],
   "source": [
    "# #Source Inputs\n",
    "# dbutils.widgets.removeAll()\n",
    "# dbutils.widgets.text(\"Source_SFTP_Access\", \"\")\n",
    "# dbutils.widgets.text(\"Source_SFTP_File_Path\", \"\")\n",
    "\n",
    "# #Target Inputs\n",
    "# dbutils.widgets.text(\"Target_S3_Access\", \"\")\n",
    "# dbutils.widgets.text(\"Target_S3_File_Path\", \"\")\n",
    "# dbutils.widgets.text(\"Notification_Recipient\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5673028e-0760-4c9a-abb7-8326091cbe4e",
     "showTitle": true,
     "title": "Loading Configs"
    }
   },
   "outputs": [],
   "source": [
    "%run ./configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86f88b0-b6b8-4bef-83ed-ec2b16698a61",
     "showTitle": true,
     "title": "Validating Inputs"
    }
   },
   "outputs": [],
   "source": [
    "# Reading user inputs\n",
    "source_access = getArgument('Source_SFTP_Access').strip()\n",
    "source_sftp_path = getArgument('Source_SFTP_File_Path').strip()\n",
    "target_access = getArgument('Target_S3_Access').strip()\n",
    "target_s3_path = getArgument('Target_S3_File_Path').strip()\n",
    "notify_recipient = getArgument('Notification_Recipient').strip()\n",
    "\n",
    "sftp_conn ={}\n",
    "s3_conn ={}\n",
    "\n",
    "# Validating user inputs\n",
    "if source_access == '':\n",
    "  dbutils.notebook.exit(\"Source SFTP Access name missing!\")\n",
    "elif source_sftp_path == '':\n",
    "  dbutils.notebook.exit(\"Source File info missing!\")\n",
    "elif target_access == '':\n",
    "  dbutils.notebook.exit(\"Target S3 Access Name missing!\")\n",
    "elif target_s3_path == '':\n",
    "  dbutils.notebook.exit(\"Target S3 File Path missing!\")\n",
    "\n",
    "if source_access in sftp_access_list: \n",
    "  sftp_conn['host'] = sftp_hosts[source_access]\n",
    "  sftp_conn['username'] = sftp_username[source_access]\n",
    "  sftp_conn['password'] = sftp_password[source_access]\n",
    "else:\n",
    "  dbutils.notebook.exit(\"Invalid Source SFTP Access!\")\n",
    "\n",
    "if target_access in s3_access_list: \n",
    "  s3_conn['s3_access_key'] = s3_access_keys[target_access]\n",
    "  s3_conn['s3_secret_key'] = s3_secret_keys[target_access]\n",
    "else:\n",
    "  dbutils.notebook.exit(\"Invalid Target S3 Access Name!\")\n",
    "service = 'SFTP-S3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01234172-0693-414a-93be-b4816348974d",
     "showTitle": true,
     "title": "Importing Required libraries"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysftp\r\n  Using cached pysftp-0.2.9-py3-none-any.whl\r\nCollecting paramiko>=1.17\r\n  Using cached paramiko-3.4.0-py3-none-any.whl (225 kB)\r\nRequirement already satisfied: cryptography>=3.3 in /databricks/python3/lib/python3.9/site-packages (from paramiko>=1.17->pysftp) (3.4.8)\r\nCollecting pynacl>=1.5\r\n  Using cached PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\r\nCollecting bcrypt>=3.2\r\n  Using cached bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\r\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=3.3->paramiko>=1.17->pysftp) (1.15.0)\r\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=1.17->pysftp) (2.21)\r\nInstalling collected packages: pynacl, bcrypt, paramiko, pysftp\r\nSuccessfully installed bcrypt-4.1.2 paramiko-3.4.0 pynacl-1.5.0 pysftp-0.2.9\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-5dbb50c4-d90b-478c-ab09-35a1013e7378/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import datetime, time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "!pip install pysftp\n",
    "import pysftp\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ddd5aa4-44bd-41f6-b820-75fb9709a874",
     "showTitle": true,
     "title": "Generating Run Identifier"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1709fe2a-c766-487b-b504-ca9535db0040\n"
     ]
    }
   ],
   "source": [
    "run_id = uuid.uuid4()\n",
    "print('run_id:',run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b81250-1a09-4ec9-8b61-f0b64f1c10d2",
     "showTitle": true,
     "title": "SQL Query Executor"
    }
   },
   "outputs": [],
   "source": [
    "def run_logger(service,log_op, opn,srcfilename, rc, target,status):\n",
    "  \"\"\"\n",
    "  Inputs: SQL Query\n",
    "  Output: Returns True if success\n",
    "  \"\"\"\n",
    "  logging.info(f\"Updating run log table for {opn} operation.....\")\n",
    "  if log_op == 'insert' :\n",
    "    query = f\"INSERT INTO TABLE run_log VALUES('{service}','{run_id}','{source_access}','{source_sftp_path}','{srcfilename}','{opn}',{rc},'{target_access}','{target}','{status}',current_timestamp())\"\n",
    "  elif log_op == 'update':\n",
    "    query = f\"UPDATE run_log SET status='{status}' where run_id = '{run_id}' and operation='{opn}'\"\n",
    "  # Executing SQL Query\n",
    "  spark.sql(query)\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec93e0a5-229f-4240-ba97-bdf776a186eb",
     "showTitle": true,
     "title": "Function to check File pattern at SFTP"
    }
   },
   "outputs": [],
   "source": [
    "def sftp_file_pattern_check(sftp_conn, source_sftp_path):\n",
    "  \"\"\"\n",
    "  Inputs: Sftp connection details, Sftp Directory\n",
    "  Output: Return file with max timestamp of pattern present in Directory\n",
    "  \"\"\"\n",
    "  logging.info('Executing File pattern check......')\n",
    "\n",
    "  sftp_parts_1 = source_sftp_path.split('/')\n",
    "  prefix = '/'.join(sftp_parts_1[:-1])\n",
    "  sftp_file_part = sftp_parts_1[-1]\n",
    "  sftp_parts_2 = sftp_file_part.split('_')\n",
    "  file_name_pattern = '_'.join(sftp_parts_2[:-1])\n",
    "  file_format = source_sftp_path.split('.')[-1]\n",
    "\n",
    "  # Create pysftp CnOpts object to handle known host keys\n",
    "  cnopts = pysftp.CnOpts()\n",
    "  cnopts.hostkeys = None  # Disable host key checking\n",
    "\n",
    "  hostname = sftp_conn['host']\n",
    "  username = sftp_conn['username']\n",
    "  password = sftp_conn['password'] \n",
    "\n",
    "  # Connect to the SFTP server\n",
    "  with pysftp.Connection(host=hostname, username=username, password=password, port=22, cnopts=cnopts) as sftp:\n",
    "    print('Connection successful to SFTP .........')\n",
    "    #Change to the specified directory\n",
    "    sftp.chdir(prefix)\n",
    "    #List files in the directory\n",
    "    objs = sftp.listdir()\n",
    "\n",
    "  match_list = []\n",
    "  # listing files with pattern match\n",
    "  if 'yyyymmddhhmmss' in source_sftp_path :\n",
    "    pattern = rf\"{file_name_pattern}_\\d{{14}}\\.{file_format}\"\n",
    "    for obj in objs :\n",
    "      if re.match(pattern, obj):\n",
    "        match_list.append(obj)\n",
    "  elif 'yyyymmdd' in source_sftp_path :\n",
    "    pattern = rf\"{file_name_pattern}_\\d{{8}}\\.{file_format}\"\n",
    "    for obj in objs:\n",
    "      if re.match(pattern, obj):\n",
    "        match_list.append(obj)\n",
    "\n",
    "  print('match_list:',match_list)\n",
    "  if len(match_list) == 0:\n",
    "    logging.info(f\"No files found at {source_sftp_path} with input pattern\")\n",
    "    #run_logger('SFTP-S3','insert','copy',0,'','failed')\n",
    "    return None\n",
    "  else:\n",
    "    # Selecting latest timestamp file for return\n",
    "    latest_file = max(match_list)\n",
    "    print('latest file:',latest_file)\n",
    "    logging.info(f\"Latest file for pattern - {pattern} at {prefix}: {latest_file}\")\n",
    "    return latest_file\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480c3b7f-3ee9-4dbd-ba76-cb17e518b0fb",
     "showTitle": true,
     "title": "Function to move and rename file at target S3"
    }
   },
   "outputs": [],
   "source": [
    "def move_and_rename_file_in_s3(s3_conn, target_s3, new_file_name):\n",
    "  \"\"\"\n",
    "  This function moves the s3 file & renames it to required file name\n",
    "  Inputs: S3 connection details, s3_path, file name\n",
    "  Outputs: True\n",
    "  \"\"\"\n",
    "\n",
    "  access_key = s3_conn['s3_access_key']\n",
    "  secret_key = s3_conn['s3_secret_key']\n",
    "  \n",
    "  s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "  s3_parts = target_s3.split('/')\n",
    "  bucket_name = s3_parts[2]\n",
    "  if target_s3.endswith(('.csv','.txt','.parquet')):\n",
    "    prefix = '/'.join(s3_parts[3:-1])\n",
    "  else:\n",
    "    prefix = '/'.join(s3_parts[3:])\n",
    "  \n",
    "  if prefix.endswith('/'):\n",
    "    folder_prefix = prefix + new_file_name + '/'\n",
    "  else :\n",
    "    folder_prefix = prefix +'/' +new_file_name+'/'\n",
    "\n",
    "  fformat = '.'+new_file_name.split('.')[-1]\n",
    "  if fformat in ['.csv','.txt']:\n",
    "    fformat = '.csv'\n",
    "  else:\n",
    "    fformat = fformat\n",
    "  # List objects in the folder\n",
    "  response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix)\n",
    "  # Retrieve the filenames from the list of objects\n",
    "  csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith(f'{fformat}')]\n",
    "  if csv_files:\n",
    "    # Pick the last csv file\n",
    "    last_csv_file = csv_files[-1]\n",
    "    # Move the file to upper directory\n",
    "    if prefix.endswith('/') :\n",
    "      key = prefix + new_file_name\n",
    "    else:\n",
    "      key = prefix + '/' + new_file_name\n",
    "    s3.copy_object(Bucket=bucket_name, CopySource=f\"{bucket_name}/{last_csv_file}\", Key= key)\n",
    "    # Delete original directory\n",
    "    s3 = boto3.resource('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "    bucket = s3.Bucket(f'{bucket_name}')\n",
    "    for obj in bucket.objects.filter(Prefix= f'{folder_prefix}'):\n",
    "      s3.Object(bucket.name,obj.key).delete()\n",
    "    logging.info(\"Moved and Renamed files\")\n",
    "    return True\n",
    "  else :\n",
    "    logging.info(\"Error during moving & renaming files\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f708dad-9c25-4d3c-b146-a38709118139",
     "showTitle": true,
     "title": "Function to read file at SFTP"
    }
   },
   "outputs": [],
   "source": [
    "def sftp_get_file(sftp_conn, source_sftp_path):\n",
    "\n",
    "  \"\"\"\n",
    "  This function reads file from SFTP location and writes a single file to DBFS local\n",
    "  Inputs: SFTP connection details, source file path\n",
    "  Ouput: Returns local DBFS path\n",
    "  \"\"\"\n",
    " \n",
    "  sftp_parts_1 = source_sftp_path.split('/')\n",
    "  prefix = '/'.join(sftp_parts_1[:-1])\n",
    "  sftp_file_part = sftp_parts_1[-1]\n",
    "  sftp_parts_2 = sftp_file_part.split('_')\n",
    "  file_name_pattern = '_'.join(sftp_parts_2[:-1])\n",
    "  file_format = source_sftp_path.split('.')[-1]\n",
    "  \n",
    "  hostname = sftp_conn['host']\n",
    "  username = sftp_conn['username']\n",
    "  password = sftp_conn['password']\n",
    "\n",
    "  # Create pysftp CnOpts object to handle known host keys\n",
    "  cnopts = pysftp.CnOpts()\n",
    "  cnopts.hostkeys = None  # Disable host key checking\n",
    "  if source_sftp_path.endswith(('.txt','.csv', '.parquet')) :\n",
    "    if ('yyyymmdd' in source_sftp_path) or ('yyyymmddHHMMSS' in source_sftp_path):\n",
    "      latest_file = sftp_file_pattern_check(sftp_conn, source_sftp_path)\n",
    "      if latest_file :\n",
    "        remote_path = prefix+'/'+ latest_file\n",
    "      else:\n",
    "        logging.error(\"No latest file found\")\n",
    "        run_logger('SFTP-S3','insert','copy',0,'','failed')\n",
    "        return None      \n",
    "    else:\n",
    "      latest_file = source_sftp_path.split('/')[-1]\n",
    "      \n",
    "    local_temp_dir = 'dbfs:/FileStore/sftp_store/'\n",
    "    if not os.path.exists(local_temp_dir):\n",
    "      os.makedirs(local_temp_dir)\n",
    "    local_path = local_temp_dir + latest_file\n",
    "    # Connect to the SFTP server\n",
    "    with pysftp.Connection(host=hostname, username=username, password=password, port=22, cnopts=cnopts) as sftp:\n",
    "      print('Connection successful to SFTP .........')\n",
    "      #Change to the specified directory\n",
    "      sftp.chdir(prefix)\n",
    "      #Downloading file to temp directory\n",
    "      remote_file_path = sftp.getcwd() + '/' + latest_file\n",
    "      with sftp.open(remote_file_path) as remote_file:\n",
    "        df = pd.read_csv(remote_file)\n",
    "        rc = len(df)\n",
    "        csv_data = df.to_csv(index=False)\n",
    "        # Save the CSV data to DBFS\n",
    "        dbutils.fs.put(local_path, csv_data, overwrite=True)\n",
    "\n",
    "      #sftp.get(remote_file_path, dbfs_file_path)\n",
    "      run_logger('SFTP-S3','insert','read',latest_file ,rc,target_s3_path,'success')\n",
    "      logging.info(f\"{latest_file} written to local dbfs directory {local_temp_dir} successfully\")\n",
    "      return local_path\n",
    "  else:\n",
    "    current_time = datetime.now()\n",
    "    timestamp = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    local_temp_dir = 'dbfs:/FileStore/sftp_store/' \n",
    "    if not os.path.exists(local_temp_dir):\n",
    "      os.makedirs(local_temp_dir)\n",
    "    with pysftp.Connection(host=hostname, username=username, password=password, port=22, cnopts=cnopts) as sftp:\n",
    "      print('Connection successful to SFTP .........')\n",
    "      #Change to the specified directory\n",
    "      sftp.chdir(source_sftp_path)\n",
    "      #Downloading file to temp directory\n",
    "      #sftp.get(source_sftp_path, local_path)\n",
    "\n",
    "      #List files in the directory\n",
    "      objs = sftp.listdir()\n",
    "\n",
    "      if len(objs) == 0:\n",
    "        logging.info(f\"No files found at {source_sftp_path}!\")\n",
    "        run_logger('SFTP-S3','insert','read','',0,target_s3_path,'failed')\n",
    "        return None\n",
    "      else:\n",
    "        tgt_file_name = str(run_id)+'_'+str(timestamp)+'.csv'\n",
    "        local_path = local_temp_dir + tgt_file_name\n",
    "        dfs = []\n",
    "        for file_name in objs:\n",
    "          remote_file_path = sftp.getcwd() + '/' + file_name\n",
    "          with sftp.open(remote_file_path) as remote_file:\n",
    "            df = pd.read_csv(remote_file)\n",
    "            dfs.append(df)\n",
    "        final_df = pd.concat(dfs,ignore_index=True)\n",
    "        rc = len(final_df)  \n",
    "        csv_data = final_df.to_csv(index=False)\n",
    "        # Save the CSV data to DBFS\n",
    "        dbutils.fs.put(local_path, csv_data, overwrite=True)\n",
    "        run_logger('SFTP-S3','insert','read','MultipleFiles',rc,target_s3_path,'success')     \n",
    "        logging.info(f\"Files written to {local_path} successfully\")\n",
    "        return local_path  \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68b1c63-06e8-46e8-adf6-1a81fc5e2e5d",
     "showTitle": true,
     "title": "Function to write file at S3"
    }
   },
   "outputs": [],
   "source": [
    "def local_s3_transfer(s3_conn,local_path, target_s3_path):\n",
    "\n",
    "  \"\"\"\n",
    "  This function reads from DBFS local and writes to target S3\n",
    "  Inputs: DBFS local path, S3 connection details, target path\n",
    "  Output: Final S3 object key\n",
    "  \"\"\"\n",
    "\n",
    "  s3_parts_1 = target_s3_path.split('/')\n",
    "  bucket_name = s3_parts_1[2]\n",
    "  prefix = '/'.join(s3_parts_1[3:-1])\n",
    "  key = '/'.join(s3_parts_1[3:])\n",
    "\n",
    "  access_key = s3_conn['s3_access_key']\n",
    "  secret_key = s3_conn['s3_secret_key']\n",
    "\n",
    "  # Setting Spark configs to access S3\n",
    "  sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", access_key)\n",
    "  sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", secret_key)  \n",
    "\n",
    "  # Initialize S3 client with credentials\n",
    "  s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)  \n",
    "\n",
    "  current_time = datetime.now()\n",
    "  timestamp = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "  current_day = datetime.today().date()\n",
    "  date = current_day.strftime(\"%Y%m%d\")  \n",
    "\n",
    "  if target_s3_path.endswith(('.csv','.txt')):\n",
    "    target_file_name = target_s3_path.split('/')[-1]\n",
    "    if ('_yyyymmddHHMMSS' in target_file_name) :\n",
    "      file_parts = target_file_name.split('_yyyymmddHHMMSS')\n",
    "      file_name = file_parts[0]+'_'+timestamp+file_parts[1]\n",
    "      target_object_key = '/'.join(s3_parts_1[3:-1])\n",
    "      target_object_key = target_object_key + '/'+ file_name\n",
    "      file_path = 's3://'+bucket_name+'/'+target_object_key\n",
    "    elif ('_yyyymmdd' in target_file_name) :\n",
    "      file_parts = target_file_name.split('_yyyymmdd')\n",
    "      file_name = file_parts[0]+'_'+date+file_parts[1]\n",
    "      target_object_key = '/'.join(s3_parts_1[3:-1])\n",
    "      target_object_key = target_object_key + '/'+ file_name\n",
    "      file_path = 's3://'+bucket_name+'/'+target_object_key\n",
    "    else:\n",
    "      target_object_key = key\n",
    "      file_path = 's3://'+bucket_name+'/'+target_object_key\n",
    "      file_name = target_object_key.split('/')[-1]\n",
    "  else:\n",
    "    file_name = local_path.split('/')[-1]\n",
    "    target_object_key = prefix+ '/'+ file_name\n",
    "    file_path = 's3://'+bucket_name+'/'+target_object_key\n",
    "\n",
    "  file_format = file_name.split('.')[-1]\n",
    "\n",
    "  if file_format == 'txt':\n",
    "    delimiter = '\\t'\n",
    "  else:\n",
    "    delimiter = ','\n",
    "  try:\n",
    "    input_df = spark.read.csv(local_path)\n",
    "    rc = input_df.count()\n",
    "    input_df.coalesce(1).write.format('csv').option('header','False').option(\"delimiter\",delimiter).mode('overwrite').save(file_path)\n",
    "    res = move_and_rename_file_in_s3(s3_conn, target_s3_path, file_name)\n",
    "    run_logger('SFTP-S3','insert','write',file_name,rc, file_path,'success')\n",
    "    logging.info(f\"File transfer successful to {bucket_name} with key {target_object_key}\")\n",
    "    return target_object_key\n",
    "  except Exception as e:\n",
    "    run_logger('SFTP-S3','insert','write',file_name,0,target_s3_path,'failed')\n",
    "    logging.error(f\"Unable to write to S3: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e44a471-3551-4caf-8785-0fe4014a64b7",
     "showTitle": true,
     "title": "Main function"
    }
   },
   "outputs": [],
   "source": [
    "def sftp_s3(sftp_conn,source_sftp_path,s3_conn,target_s3_path):\n",
    "  local_path = sftp_get_file(sftp_conn, source_sftp_path)\n",
    "  if local_path :\n",
    "    res = local_s3_transfer(s3_conn,local_path, target_s3_path)\n",
    "    return res\n",
    "  else:\n",
    "    run_logger('SFTP-S3','insert','write','',0,target_s3_path,'failed')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad7707b-6667-446d-891e-86cf3efe1ee4",
     "showTitle": true,
     "title": "Calling Main function"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-5dbb50c4-d90b-478c-ab09-35a1013e7378/lib/python3.9/site-packages/pysftp/__init__.py:61: UserWarning: Failed to load HostKeys from /root/.ssh/known_hosts.  You will need to explicitly load HostKeys (cnopts.hostkeys.load(filename)) or disableHostKey checking (cnopts.hostkeys = None).\n  warnings.warn(wmsg, UserWarning)\nINFO:root:Executing File pattern check......\nINFO:paramiko.transport:Connected (version 2.0, client AzureSSH_1.0.0)\nINFO:paramiko.transport:Authentication (password) successful!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful to SFTP .........\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paramiko.transport.sftp:[chan 0] Opened sftp connection (server version 3)\nINFO:paramiko.transport.sftp:[chan 0] sftp session closed.\nINFO:root:Latest file for pattern - red_\\d{14}\\.csv at /Home/Incoming: red_20260101121212.csv\nINFO:py4j.clientserver:Closing down clientserver connection\nINFO:paramiko.transport:Connected (version 2.0, client AzureSSH_1.0.0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_list: ['red_20240101121212.csv', 'red_20260101121212.csv']\nlatest file: red_20260101121212.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paramiko.transport:Authentication (password) successful!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful to SFTP .........\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paramiko.transport.sftp:[chan 0] Opened sftp connection (server version 3)\nINFO:root:Updating run log table for read operation.....\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 29829 bytes.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:red_20260101121212.csv written to local dbfs directory dbfs:/FileStore/sftp_store/ successfully\nINFO:paramiko.transport.sftp:[chan 0] sftp session closed.\nINFO:root:Moved and Renamed files\nINFO:root:Updating run log table for write operation.....\nINFO:root:File transfer successful to sdevalla-portfolio with key orch_test/sftp_20240323.txt\nINFO:root:File transfer Successful\n"
     ]
    }
   ],
   "source": [
    "status = sftp_s3(sftp_conn,source_sftp_path,s3_conn,target_s3_path)\n",
    "\n",
    "# Checking status\n",
    "if status :\n",
    "  logging.info(f\"File transfer Successful\")\n",
    "else:\n",
    "  logging.info(\"Failed to transfer file from SFTP to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2d4aac-95b3-4639-b4b0-e0e7145a68d5",
     "showTitle": true,
     "title": "Function to send email notification"
    }
   },
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "def send_email(subject, body_html, sender, recipients):\n",
    "\n",
    "  \"\"\"\n",
    "  This function sends email notification to recipeints on run status\n",
    "  Inputs: Email content, receiver email address list\n",
    "  Output: Success/Failure message\n",
    "  \"\"\"\n",
    "  access_key = ses_conn['access_key']\n",
    "  secret_key = ses_conn['secret_key']\n",
    "\n",
    "  #Creating boto3 ses client\n",
    "  ses_client = boto3.client('ses', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name = 'us-east-2')\n",
    "\n",
    "  # Create a MIME message\n",
    "  body_text = \"This email requires HTML support. Please view in a HTML-compatible email client.\"\n",
    "  charset = \"UTF-8\"\n",
    "  \n",
    "  # Assemble the email\n",
    "  try:\n",
    "      response = ses_client.send_email(\n",
    "          Destination={\n",
    "              'ToAddresses': recipients,\n",
    "          },\n",
    "          Message={\n",
    "              'Body': {\n",
    "                  'Html': {\n",
    "                      'Charset': charset,\n",
    "                      'Data': body_html,\n",
    "                  },\n",
    "                  'Text': {\n",
    "                      'Charset': charset,\n",
    "                      'Data': body_text,\n",
    "                  },\n",
    "              },\n",
    "              'Subject': {\n",
    "                  'Charset': charset,\n",
    "                  'Data': subject,\n",
    "              },\n",
    "          },\n",
    "          Source=sender,\n",
    "      )\n",
    "  except ClientError as e:\n",
    "      print(e.response['Error']['Message'])\n",
    "  else:\n",
    "      print(\"Email sent! Message ID:\", response['MessageId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71da7aa8-dbd0-4642-8d19-5ebcd978fd2b",
     "showTitle": true,
     "title": "Function to create HTML Body"
    }
   },
   "outputs": [],
   "source": [
    "def dataframe_to_html_table(df):\n",
    "    # Convert DataFrame to HTML table\n",
    "    html_table = df.to_html(index=False)\n",
    "    # Add inline CSS styling to color the header\n",
    "    styled_header = '<th style=\"background-color: #FB451D; color: white;\">'\n",
    "    html_table = html_table.replace('<th>', styled_header)\n",
    "    # Format the HTML table\n",
    "    formatted_html_table = f'<html><body>{html_table}</body></html>'\n",
    "    return formatted_html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82b9333-e5ac-4dd9-833c-48d927bff56a",
     "showTitle": true,
     "title": "Calling Email Notification Fucntion"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><p>Hi,</p><p>Please find the status of service: SFTP-S3 with run id: 1709fe2a-c766-487b-b504-ca9535db0040</p><html><body><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Service</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Source SFTP</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Source File</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Task</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Record_Count</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Target_S3</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Status</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>SFTP-S3</td>\n",
       "      <td>/Home/Incoming/red_yyyymmddhhmmss.csv</td>\n",
       "      <td>red_20260101121212.csv</td>\n",
       "      <td>read</td>\n",
       "      <td>918</td>\n",
       "      <td>s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt</td>\n",
       "      <td>success</td>\n",
       "      <td>2024-03-23 16:13:58.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SFTP-S3</td>\n",
       "      <td>/Home/Incoming/red_yyyymmddhhmmss.csv</td>\n",
       "      <td>sftp_20240323.txt</td>\n",
       "      <td>write</td>\n",
       "      <td>919</td>\n",
       "      <td>s3://sdevalla-portfolio/orch_test/sftp_20240323.txt</td>\n",
       "      <td>success</td>\n",
       "      <td>2024-03-23 16:14:08.636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></body></html></body></html>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<html><body><p>Hi,</p><p>Please find the status of service: SFTP-S3 with run id: 1709fe2a-c766-487b-b504-ca9535db0040</p><html><body><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th style=\"background-color: #FB451D; color: white;\">Service</th>\n      <th style=\"background-color: #FB451D; color: white;\">Source SFTP</th>\n      <th style=\"background-color: #FB451D; color: white;\">Source File</th>\n      <th style=\"background-color: #FB451D; color: white;\">Task</th>\n      <th style=\"background-color: #FB451D; color: white;\">Record_Count</th>\n      <th style=\"background-color: #FB451D; color: white;\">Target_S3</th>\n      <th style=\"background-color: #FB451D; color: white;\">Status</th>\n      <th style=\"background-color: #FB451D; color: white;\">Timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>SFTP-S3</td>\n      <td>/Home/Incoming/red_yyyymmddhhmmss.csv</td>\n      <td>red_20260101121212.csv</td>\n      <td>read</td>\n      <td>918</td>\n      <td>s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt</td>\n      <td>success</td>\n      <td>2024-03-23 16:13:58.907</td>\n    </tr>\n    <tr>\n      <td>SFTP-S3</td>\n      <td>/Home/Incoming/red_yyyymmddhhmmss.csv</td>\n      <td>sftp_20240323.txt</td>\n      <td>write</td>\n      <td>919</td>\n      <td>s3://sdevalla-portfolio/orch_test/sftp_20240323.txt</td>\n      <td>success</td>\n      <td>2024-03-23 16:14:08.636</td>\n    </tr>\n  </tbody>\n</table></body></html></body></html>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No recipients to send email!\n"
     ]
    }
   ],
   "source": [
    "subject = f\"{service} run status for run id - {run_id}\"\n",
    "df = spark.sql(\"\"\" select distinct service as Service, source_path_table as `Source SFTP`, source_file_dml as `Source File`, operation as Task,  record_count as Record_Count, target_path_table as Target_S3, Status, Timestamp from run_log where run_id = '{}' order by task \"\"\".format(run_id ))\n",
    "html_table = dataframe_to_html_table(df.toPandas())\n",
    "body_html = f\"<html><body><p>Hi,</p><p>Please find the status of service: {service} with run id: {run_id}</p>{html_table}</body></html>\"\n",
    "displayHTML(body_html)\n",
    "sender = \"noreplyd22snotification@gmail.com\"\n",
    "if notify_recipient != '' :\n",
    "  recipients = notify_recipient.split(',')\n",
    "else:\n",
    "   recipients = []\n",
    "if len(recipients) > 0:\n",
    "  send_email(subject, body_html, sender, recipients)\n",
    "else:\n",
    "  print('No recipients to send email!')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3497264830918302,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Source_SFTP_Access",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Source_SFTP_File_Path",
      "width": 529
     },
     {
      "breakBefore": false,
      "name": "Target_S3_Access",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Target_S3_File_Path",
      "width": 529
     },
     {
      "breakBefore": false,
      "name": "Notification_Recipient",
      "width": 1437
     }
    ]
   },
   "notebookName": "SFTP-S3",
   "widgets": {
    "Notification_Recipient": {
     "currentValue": "",
     "nuid": "c9aa8ffc-64ec-48ff-ac4b-d10b741f9ad1",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Notification_Recipient",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Source_SFTP_Access": {
     "currentValue": "sftp_sd",
     "nuid": "4d9cace5-3a3b-46ac-8a8e-c9ea4b74b466",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_SFTP_Access",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Source_SFTP_File_Path": {
     "currentValue": "/Home/Incoming/red_yyyymmddhhmmss.csv",
     "nuid": "2bd5774d-19c6-4ec3-8977-88700d8eb77f",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_SFTP_File_Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_S3_Access": {
     "currentValue": "sd_s3",
     "nuid": "8a74e973-2757-48b0-8525-9cdc69270283",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_S3_Access",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_S3_File_Path": {
     "currentValue": "s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt",
     "nuid": "85b5fe9d-db5b-4b96-aa29-ea7012595a29",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_S3_File_Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
