{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b90eda-2366-415e-8c1a-c0ceefb3336c",
     "showTitle": true,
     "title": "Input Parameter Requirements"
    }
   },
   "outputs": [],
   "source": [
    "# Source DB: Defaults to MySQL\n",
    "# Source DB Access Name: Required\n",
    "# Source DB Table: Required [schema_name.table_name] [If multiple tables used in DML, fill first table name]\n",
    "# DML: Defaults to: SELECT * FROM Source-Table\n",
    "# Target S3 Access Name: Required\n",
    "# Target S3 File Path: Required [Formats allowed: s3://bucket/prefix/filename.txt, s3://bucket/prefix/filename_yyyymmdd.txt, _yyyymmddHHMMSS.txt, \n",
    "# Defaults to table_name_yyyymmddHHMMSS.txt ]\n",
    "# Target File Delimiter: Defaults to comma [comma: ',' tab: '\\t', 'pipe': '|']\n",
    "# Email Notification: Defaults to skip [Requires email id to send notification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daacaca5-9648-47c2-a325-a0f12d3deccd",
     "showTitle": true,
     "title": "Creating Widgets"
    }
   },
   "outputs": [],
   "source": [
    "# #Source Inputs\n",
    "# dbutils.widgets.removeAll()\n",
    "# dbutils.widgets.dropdown(\"Source_DB\", \"MySQL\", [\"MySQL\", \"PSQL\"])\n",
    "# dbutils.widgets.text(\"Source_DB_Access\", \"\")\n",
    "# dbutils.widgets.text(\"Source_DB_Table\", \"\")\n",
    "# dbutils.widgets.text(\"DML\", \"\")\n",
    "\n",
    "# #Target Inputs\n",
    "# dbutils.widgets.text(\"Target_S3_Access\", \"\")\n",
    "# dbutils.widgets.text(\"Target_S3_File_Path\", \"\")\n",
    "# dbutils.widgets.text(\"Target_File_Delimiter\", \",\")\n",
    "# dbutils.widgets.text(\"Notification_Recipient\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5482732c-8607-4306-b6d5-a48ae57efb5d",
     "showTitle": true,
     "title": "Loading Configs"
    }
   },
   "outputs": [],
   "source": [
    "%run ./configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a143be-c24e-4403-b43c-77a61819cdaf",
     "showTitle": true,
     "title": "Validating Inputs"
    }
   },
   "outputs": [],
   "source": [
    "# Reading user inputs\n",
    "source_db = getArgument('Source_DB').strip()\n",
    "source_access = getArgument('Source_DB_Access').strip()\n",
    "source_table = getArgument('Source_DB_Table').strip()\n",
    "DML = getArgument('DML').strip()\n",
    "target_access = getArgument('Target_S3_Access').strip()\n",
    "target_s3 = getArgument('Target_S3_File_Path').strip()\n",
    "delimiter = getArgument('Target_File_Delimiter').strip()\n",
    "notify_recipient = getArgument('Notification_Recipient').strip()\n",
    "\n",
    "s3_conn ={}\n",
    "db_conn ={}\n",
    "\n",
    "# Validating user inputs\n",
    "if source_access == '':\n",
    "  dbutils.notebook.exit(\"Source DB Access name missing!\")\n",
    "elif source_table == '':\n",
    "  dbutils.notebook.exit(\"Source Table info missing!\")\n",
    "elif len(source_table.split('.')) != 2:\n",
    "  dbutils.notebook.exit(\"Schema name missing in source table!\")\n",
    "elif target_access == '':\n",
    "  dbutils.notebook.exit(\"Target S3 Access Name missing!\")\n",
    "elif target_s3 == '':\n",
    "  dbutils.notebook.exit(\"Target S3 File Path missing!\")\n",
    "\n",
    "if target_s3.endswith(('.csv','.txt','.parquet')):\n",
    "  target_file_name = target_s3.split('/')[-1]\n",
    "else:\n",
    "  target_file_name = ''\n",
    "\n",
    "if source_access in db_access_list:\n",
    "  db_conn['db_host'] = db_hosts[source_access]\n",
    "  db_conn['db_username'] = db_usernames[source_access]\n",
    "  db_conn['db_password'] = db_passwords[source_access]\n",
    "else:\n",
    "  dbutils.notebook.exit(\"Invalid DB Access Name!\")\n",
    "\n",
    "if target_access in s3_access_list: \n",
    "  s3_conn['s3_access_key'] = s3_access_keys[target_access]\n",
    "  s3_conn['s3_secret_key'] = s3_secret_keys[target_access]\n",
    "else:\n",
    "  dbutils.notebook.exit(\"Invalid S3 Access Name!\")\n",
    "service = 'DB-S3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f88b399-a786-4537-9a8f-369e3fdfd0b6",
     "showTitle": true,
     "title": "Importing Required libraries"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import datetime, time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3418b4bf-7b7f-42a5-8fb4-5d1b9c319f12",
     "showTitle": true,
     "title": "Generating Run Identifier"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: a394b018-597c-4f3d-b93a-091b8db07145\n"
     ]
    }
   ],
   "source": [
    "run_id = uuid.uuid4()\n",
    "print('run_id:',run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce6e823-2084-4720-bf69-27526a84d3d9",
     "showTitle": true,
     "title": "SQL Query Executor function"
    }
   },
   "outputs": [],
   "source": [
    "def run_logger(service,log_op, opn, DML, rc, target,status):\n",
    "  \"\"\"\n",
    "  Inputs: SQL Query\n",
    "  Output: Returns True if success\n",
    "  \"\"\"\n",
    "  logging.info(f\"Updating run log table for {opn} operation.....\")\n",
    "  if log_op == 'insert' :\n",
    "    query = f\"INSERT INTO TABLE run_log VALUES('{service}','{run_id}','{source_access}','{source_table}','{DML}','{opn}',{rc},'{target_access}','{target}','{status}',current_timestamp())\"\n",
    "  elif log_op == 'update':\n",
    "    query = f\"UPDATE run_log SET status='{status}' where run_id = '{run_id}' and operation='{opn}'\"\n",
    "  # Executing SQL Query\n",
    "  spark.sql(query)\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce86e5e6-c7af-4ddb-ab86-e38150fa32af",
     "showTitle": true,
     "title": "Function to read data from source table"
    }
   },
   "outputs": [],
   "source": [
    "def table_data_read(db_conn, source_table, DML):\n",
    "  \"\"\"\n",
    "  Reads data from table using DML if DML is blank SELECT * will be used\n",
    "  Inputs: DB connection details, table name, DML\n",
    "  Output: returns dataframe, record count\n",
    "  \"\"\"\n",
    "\n",
    "  # Reading user input DML\n",
    "  if DML == '':\n",
    "    query = f\"(SELECT * FROM {source_table})as query\"\n",
    "  else:\n",
    "    query = '( '+ DML+' ) as query'\n",
    "  \n",
    "  print('Executable Query:',query)\n",
    "\n",
    "  db_name = source_table.split('.')[0]\n",
    "  dbtable = source_table.split('.')[1]\n",
    "\n",
    "  # Preparing url string for JDBC connection\n",
    "  if source_db == 'MySQL':\n",
    "    url = f\"jdbc:mysql://{db_conn['db_host']}:3306/{db_name}\" \n",
    "  elif source_db == 'PSQL':\n",
    "    url = f\"jdbc:postgresql://{db_conn['db_host']}:5432/{db_name}\" \n",
    "\n",
    "  try:\n",
    "    read_df = (spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", url)\n",
    "    .option(\"dbtable\", query)\n",
    "    .option(\"user\", db_conn['db_username'])\n",
    "    .option(\"password\", db_conn['db_password'])\n",
    "    .load()\n",
    "    )\n",
    "  except Exception as e:\n",
    "    logging.error(f\"Error during reading from table: {e}\")\n",
    "    run_logger('DB-S3','insert','read', query, 0, target_s3 ,'failed')\n",
    "    return None\n",
    "  if read_df :\n",
    "    rc = read_df.count()\n",
    "    query = query.replace(\"'\", \"\\\\'\")\n",
    "    run_logger('DB-S3','insert','read', query, rc, target_s3 ,'success')\n",
    "    logging.info(\"Completed reading from table ......\")\n",
    "    return [read_df,rc]\n",
    "  else:\n",
    "    query = query.replace(\"'\", \"\\\\'\")\n",
    "    run_logger('DB-S3','insert','read', query, 0, target_s3 ,'failed')\n",
    "    logging.info(\"Error during reading from table ......\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfecde7e-b16c-4e1b-8394-c53ff4c91937",
     "showTitle": true,
     "title": "Function to move and rename file at Target S3"
    }
   },
   "outputs": [],
   "source": [
    "def move_and_rename_file_in_s3(s3_conn, target_s3, new_file_name):\n",
    "  \"\"\"\n",
    "  This function moves the s3 file & renames it to required file name\n",
    "  Inputs: S3 connection details, s3_path, file name\n",
    "  Outputs: True\n",
    "  \"\"\"\n",
    "\n",
    "  access_key = s3_conn['s3_access_key']\n",
    "  secret_key = s3_conn['s3_secret_key']\n",
    "  \n",
    "  # Creating boto3 client\n",
    "  s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "  s3_parts = target_s3.split('/')\n",
    "  bucket_name = s3_parts[2]\n",
    "  if target_s3.endswith(('.csv','.txt','.parquet')):\n",
    "    prefix = '/'.join(s3_parts[3:-1])\n",
    "  else:\n",
    "    prefix = '/'.join(s3_parts[3:])\n",
    "\n",
    "  if prefix.endswith('/'):\n",
    "    folder_prefix = prefix + new_file_name + '/'\n",
    "  else :\n",
    "    folder_prefix = prefix +'/' +new_file_name+'/'\n",
    "\n",
    "  fformat = '.'+new_file_name.split('.')[-1]\n",
    "  if fformat in ['.csv','.txt']:\n",
    "    fformat = '.csv'\n",
    "  else:\n",
    "    fformat = fformat\n",
    "\n",
    "  # List objects in the folder\n",
    "  response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix)\n",
    "  # Retrieve the filenames from the list of objects\n",
    "  csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith(f'{fformat}')]\n",
    "\n",
    "  if csv_files:\n",
    "    # Pick the last csv file\n",
    "    last_csv_file = csv_files[-1]\n",
    "    if prefix.endswith('/') :\n",
    "      key = prefix + new_file_name\n",
    "    else:\n",
    "      key = prefix + '/' + new_file_name\n",
    "    # Move the file to upper directory\n",
    "    s3.copy_object(Bucket=bucket_name, CopySource=f\"{bucket_name}/{last_csv_file}\", Key= key)\n",
    "    # Delete original directory\n",
    "    s3 = boto3.resource('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "    bucket = s3.Bucket(f'{bucket_name}')\n",
    "    for obj in bucket.objects.filter(Prefix= f'{folder_prefix}'):\n",
    "      s3.Object(bucket.name,obj.key).delete()\n",
    "    logging.info(\"Moved and Renamed files\")\n",
    "    return True\n",
    "  else :\n",
    "    logging.info(\"Error during moving & renaming files\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7505ffdd-3acb-4bda-9ca3-e208a9232a84",
     "showTitle": true,
     "title": "Function to write Dataframe to Target S3"
    }
   },
   "outputs": [],
   "source": [
    "def write_data_s3(s3_conn, target_s3, target_file_name, delimiter, input_df):\n",
    "\n",
    "  \"\"\"\n",
    "  This function writes spark df read from DB to S3 path provided\n",
    "  Inputs: S3 connection details, s3 landing path, final file name, input read df\n",
    "  Output: record count, file name\n",
    "  \"\"\"\n",
    "\n",
    "  s3_parts_1 = target_s3.split('/')\n",
    "  bucket_name = s3_parts_1[2]\n",
    "  if target_s3.endswith(('.csv','.txt','.parquet')):\n",
    "    prefix = '/'.join(s3_parts_1[3:-1])\n",
    "  else:\n",
    "    prefix = '/'.join(s3_parts_1[3:])\n",
    "\n",
    "  # Accessing keys from connection inputs\n",
    "  access_key = s3_conn['s3_access_key']\n",
    "  secret_key = s3_conn['s3_secret_key']  \n",
    "\n",
    "  # Setting Spark configs to access S3\n",
    "  sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", access_key)\n",
    "  sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", secret_key)\n",
    "\n",
    "  # Creating boto3 client\n",
    "  s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "  current_time = datetime.now()\n",
    "  timestamp = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "  current_day = datetime.today().date()\n",
    "  date = current_day.strftime(\"%Y%m%d\")\n",
    "\n",
    "  if target_file_name == '':\n",
    "    file_name = source_table.split('.')[1]+'_'+timestamp+'.csv'\n",
    "    if not target_s3.endswith('/'):\n",
    "      target_s3 = target_s3 + '/'\n",
    "    file_path = 's3://'+bucket_name+'/'+prefix+'/'+file_name\n",
    "  elif target_file_name != '':\n",
    "    if ('_yyyymmddHHMMSS' in target_file_name) :\n",
    "      file_parts = target_file_name.split('_yyyymmddHHMMSS')\n",
    "      file_name = file_parts[0]+'_'+timestamp+file_parts[1]\n",
    "      file_path = 's3://'+bucket_name+'/'+prefix+'/'+file_name\n",
    "    elif ('_yyyymmdd' in target_file_name) :\n",
    "      file_parts = target_file_name.split('_yyyymmdd')\n",
    "      file_name = file_parts[0]+'_'+date+file_parts[1]\n",
    "      file_path = 's3://'+bucket_name+'/'+prefix+'/'+file_name\n",
    "      print('file_name:',file_name)\n",
    "    else :\n",
    "      file_name = target_file_name\n",
    "      file_path = 's3://'+bucket_name+'/'+prefix+'/'+file_name\n",
    "    print('Target File name:',file_name)\n",
    "  \n",
    "  file_format = file_name.split('.')[-1]\n",
    "  rc = input_df.count()\n",
    "\n",
    "  if file_format in ['txt','csv']:\n",
    "    input_df.coalesce(1).write.format('csv').option('header','True').option(\"delimiter\",delimiter).mode('overwrite').save(file_path)\n",
    "  else:\n",
    "    input_df.write.mode('overwrite').parquet(file_path)\n",
    "  res = move_and_rename_file_in_s3(s3_conn, target_s3, file_name )\n",
    "  if res:\n",
    "    run_logger('DB-S3','insert','write','',rc,file_path, 'success')\n",
    "    logging.info(\"Completed writing file.....\")\n",
    "    return [rc, file_name]\n",
    "  else:\n",
    "    run_logger('DB-S3','insert','write','',0,target_s3, 'failed')\n",
    "    logging.info(\"Error during writing file.....\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0243ec3c-c536-4c38-8e27-dcf062056127",
     "showTitle": true,
     "title": "Main Function"
    }
   },
   "outputs": [],
   "source": [
    "def db_s3(s3_conn, source_table, DML, target_s3, target_file_name, delimiter, db_conn):\n",
    "  \n",
    "  \"\"\"\n",
    "  Main function to call read and write functions\n",
    "  Inputs: source parameters, target prameters\n",
    "  Output: record count\n",
    "  \"\"\"\n",
    "  inputs = table_data_read(db_conn, source_table, DML)\n",
    "  if inputs:\n",
    "    result = write_data_s3(s3_conn, target_s3, target_file_name, delimiter, inputs[0])\n",
    "  else:\n",
    "    #run_logger('DB-S3','update','read','','','','failed')\n",
    "    result = None\n",
    "    \n",
    "  if not result:\n",
    "    run_logger('DB-S3','update','write','','','','failed')\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2721a8f9-3b1a-4ad4-8c00-bd1f80f41ed8",
     "showTitle": true,
     "title": "Initiating main function call"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executable Query: ( select * from reddit.reddit_posts_agg limit 1000 ) as query\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Updating run log table for read operation.....\nINFO:root:Completed reading from table ......\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target File name: db_s3_20240323162918.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Moved and Renamed files\nINFO:root:Updating run log table for write operation.....\nINFO:root:Completed writing file.....\nINFO:root:1000 records transferred from DB to S3 s3://sdevalla-portfolio/orch_test/db_s3_yyyymmddhhmmss.csv with filename db_s3_20240323162918.csv\n"
     ]
    }
   ],
   "source": [
    "# Calling main function\n",
    "status = db_s3(s3_conn, source_table, DML, target_s3, target_file_name, delimiter, db_conn)\n",
    "\n",
    "# Checking status\n",
    "if status :\n",
    "  logging.info(f\"{status[0]} records transferred from DB to S3 {target_s3} with filename {status[1]}\")\n",
    "else:\n",
    "  logging.info(\"Failed to transfer file from S3 to Target table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad72451c-5251-43b9-8a64-f750b0a0a215",
     "showTitle": true,
     "title": "Function to send email notification"
    }
   },
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "def send_email(subject, body_html, sender, recipients):\n",
    "\n",
    "  \"\"\"\n",
    "  This function sends email notification to recipeints on run status\n",
    "  Inputs: Email content, receiver email address list\n",
    "  Output: Success/Failure message\n",
    "  \"\"\"\n",
    "  access_key = ses_conn['access_key']\n",
    "  secret_key = ses_conn['secret_key']\n",
    "\n",
    "  #Creating boto3 ses client\n",
    "  ses_client = boto3.client('ses', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name = 'us-east-2')\n",
    "\n",
    "  # Create a MIME message\n",
    "  body_text = \"This email requires HTML support. Please view in a HTML-compatible email client.\"\n",
    "  charset = \"UTF-8\"\n",
    "  \n",
    "  # Assemble the email\n",
    "  try:\n",
    "      response = ses_client.send_email(\n",
    "          Destination={\n",
    "              'ToAddresses': recipients,\n",
    "          },\n",
    "          Message={\n",
    "              'Body': {\n",
    "                  'Html': {\n",
    "                      'Charset': charset,\n",
    "                      'Data': body_html,\n",
    "                  },\n",
    "                  'Text': {\n",
    "                      'Charset': charset,\n",
    "                      'Data': body_text,\n",
    "                  },\n",
    "              },\n",
    "              'Subject': {\n",
    "                  'Charset': charset,\n",
    "                  'Data': subject,\n",
    "              },\n",
    "          },\n",
    "          Source=sender,\n",
    "      )\n",
    "  except ClientError as e:\n",
    "      print(e.response['Error']['Message'])\n",
    "  else:\n",
    "      print(\"Email sent! Message ID:\", response['MessageId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d60dcb-77ba-47c5-9176-e18f11b53c03",
     "showTitle": true,
     "title": "Function to Create HTML of body table"
    }
   },
   "outputs": [],
   "source": [
    "def dataframe_to_html_table(df):\n",
    "    # Convert DataFrame to HTML table\n",
    "    html_table = df.to_html(index=False)\n",
    "    # Add inline CSS styling to color the header\n",
    "    styled_header = '<th style=\"background-color: #FB451D; color: white;\">'\n",
    "    html_table = html_table.replace('<th>', styled_header)\n",
    "    # Format the HTML table\n",
    "    formatted_html_table = f'<html><body>{html_table}</body></html>'\n",
    "    return formatted_html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33adf749-078e-43f4-8705-d6ae0e847e9d",
     "showTitle": true,
     "title": "Calling Email Notification function"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><p>Hi,</p><p>Please find the status of service: DB-S3 with run id: a394b018-597c-4f3d-b93a-091b8db07145</p><html><body><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Service</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Source Table</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">DML</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Task</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Record_Count</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Target_S3</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Status</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>DB-S3</td>\n",
       "      <td>reddit.reddit_posts_agg</td>\n",
       "      <td>( select * from reddit.reddit_posts_agg limit 1000 ) as query</td>\n",
       "      <td>read</td>\n",
       "      <td>1000</td>\n",
       "      <td>s3://sdevalla-portfolio/orch_test/db_s3_yyyymmddhhmmss.csv</td>\n",
       "      <td>success</td>\n",
       "      <td>2024-03-23 16:29:14.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DB-S3</td>\n",
       "      <td>reddit.reddit_posts_agg</td>\n",
       "      <td></td>\n",
       "      <td>write</td>\n",
       "      <td>1000</td>\n",
       "      <td>s3://sdevalla-portfolio/orch_test/db_s3_20240323162918.csv</td>\n",
       "      <td>success</td>\n",
       "      <td>2024-03-23 16:29:22.805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></body></html></body></html>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<html><body><p>Hi,</p><p>Please find the status of service: DB-S3 with run id: a394b018-597c-4f3d-b93a-091b8db07145</p><html><body><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th style=\"background-color: #FB451D; color: white;\">Service</th>\n      <th style=\"background-color: #FB451D; color: white;\">Source Table</th>\n      <th style=\"background-color: #FB451D; color: white;\">DML</th>\n      <th style=\"background-color: #FB451D; color: white;\">Task</th>\n      <th style=\"background-color: #FB451D; color: white;\">Record_Count</th>\n      <th style=\"background-color: #FB451D; color: white;\">Target_S3</th>\n      <th style=\"background-color: #FB451D; color: white;\">Status</th>\n      <th style=\"background-color: #FB451D; color: white;\">Timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>DB-S3</td>\n      <td>reddit.reddit_posts_agg</td>\n      <td>( select * from reddit.reddit_posts_agg limit 1000 ) as query</td>\n      <td>read</td>\n      <td>1000</td>\n      <td>s3://sdevalla-portfolio/orch_test/db_s3_yyyymmddhhmmss.csv</td>\n      <td>success</td>\n      <td>2024-03-23 16:29:14.637</td>\n    </tr>\n    <tr>\n      <td>DB-S3</td>\n      <td>reddit.reddit_posts_agg</td>\n      <td></td>\n      <td>write</td>\n      <td>1000</td>\n      <td>s3://sdevalla-portfolio/orch_test/db_s3_20240323162918.csv</td>\n      <td>success</td>\n      <td>2024-03-23 16:29:22.805</td>\n    </tr>\n  </tbody>\n</table></body></html></body></html>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No recipients to send email!\n"
     ]
    }
   ],
   "source": [
    "subject = f\"{service} run status for run id - {run_id}\"\n",
    "df = spark.sql(\"\"\" select distinct service as Service, source_path_table as `Source Table`, source_file_dml as DML, operation as Task,  record_count as Record_Count, target_path_table as Target_S3, Status, Timestamp from run_log where run_id = '{}'  \"\"\".format(run_id ))\n",
    "html_table = dataframe_to_html_table(df.toPandas())\n",
    "body_html = f\"<html><body><p>Hi,</p><p>Please find the status of service: DB-S3 with run id: {run_id}</p>{html_table}</body></html>\"\n",
    "displayHTML(body_html)\n",
    "sender = \"noreplyd22snotification@gmail.com\"\n",
    "if notify_recipient != '' :\n",
    "  recipients = notify_recipient.split(',')\n",
    "else:\n",
    "   recipients = []\n",
    "if len(recipients) > 0:\n",
    "  send_email(subject, body_html, sender, recipients)\n",
    "else:\n",
    "  print('No recipients to send email!')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1210973057360825,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Source_DB",
      "width": 105
     },
     {
      "breakBefore": false,
      "name": "Source_DB_Access",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Source_DB_Table",
      "width": 529
     },
     {
      "breakBefore": false,
      "name": "DML",
      "width": 589
     },
     {
      "breakBefore": false,
      "name": "Target_S3_Access",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Target_S3_File_Path",
      "width": 529
     },
     {
      "breakBefore": false,
      "name": "Target_File_Delimiter",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Notification_Recipient",
      "width": 529
     }
    ]
   },
   "notebookName": "DB-S3",
   "widgets": {
    "DML": {
     "currentValue": "select * from reddit.reddit_posts_agg limit 1000",
     "nuid": "91451663-5387-45d7-a802-3f0e7a3f908c",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "DML",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Notification_Recipient": {
     "currentValue": "",
     "nuid": "c4457bb1-d6b0-457f-bcc6-c66cd2336ee2",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Notification_Recipient",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Source_DB": {
     "currentValue": "MySQL",
     "nuid": "542b4d4a-f66a-46e4-99af-13d2cac1f021",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "MySQL",
      "label": null,
      "name": "Source_DB",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "MySQL",
        "PSQL"
       ]
      }
     }
    },
    "Source_DB_Access": {
     "currentValue": "sd_rds",
     "nuid": "687fc152-d501-41fa-8cb5-eb486c8d8acc",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_DB_Access",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Source_DB_Table": {
     "currentValue": "reddit.reddit_posts_agg",
     "nuid": "dd3682b4-2b8e-4832-afa6-9d87b46f2082",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_DB_Table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_File_Delimiter": {
     "currentValue": "|",
     "nuid": "d40877a0-d6f4-40a4-98a3-2e0ab2cbe4be",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": ",",
      "label": null,
      "name": "Target_File_Delimiter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_S3_Access": {
     "currentValue": "sd_s3",
     "nuid": "70f317d5-58c0-48c5-91a9-f7e6e46a21e6",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_S3_Access",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_S3_File_Path": {
     "currentValue": "s3://sdevalla-portfolio/orch_test/db_s3_yyyymmddHMMSS.csv",
     "nuid": "d57373c3-9c6e-48c7-9a92-14e967ddc5b3",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_S3_File_Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
