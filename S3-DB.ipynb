{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b90eda-2366-415e-8c1a-c0ceefb3336c",
     "showTitle": true,
     "title": "Input Parameter Requirements"
    }
   },
   "outputs": [],
   "source": [
    "# S3 Access Name: Required\n",
    "# S3 File Path: Required [Patterns accepted: bucket/prefix/filename_yyyymmdd.txt/csv, filename_yyyymmddHHMMSS.txt/csv] [Note: If blank, all files at path will be used]\n",
    "# File Delimiter: Defaults to comma [comma: ',' tab: '\\t', 'pipe': '|']\n",
    "# Target DB: Defaults to MySQL\n",
    "# DB Access Name: Required\n",
    "# DB Target Table: Required [schema_name.table_name]\n",
    "# Load Type: Defaults to Append\n",
    "# Header Row: Defaults to True\n",
    "# Email Notification: Defaults to skip [Requires email id to send notification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daacaca5-9648-47c2-a325-a0f12d3deccd",
     "showTitle": true,
     "title": "Creating Widgets"
    }
   },
   "outputs": [],
   "source": [
    "# #Source Inputs\n",
    "# dbutils.widgets.removeAll()\n",
    "# dbutils.widgets.text(\"Source_S3_Access\", \"\")\n",
    "# dbutils.widgets.text(\"Source_S3_File_Path\", \"\")\n",
    "# dbutils.widgets.dropdown(\"Header_Row\", \"True\", [\"True\", \"False\"])\n",
    "# dbutils.widgets.text(\"File_Delimiter\", \",\")\n",
    "\n",
    "# #Target Inputs\n",
    "# dbutils.widgets.dropdown(\"Target_DB\", \"MySQL\", [\"MySQL\", \"PSQL\"])\n",
    "# dbutils.widgets.dropdown(\"Load_Type\", \"Append\", [\"Append\", \"Overwrite\"])\n",
    "# dbutils.widgets.text(\"Target_DB_Access\", \"\")\n",
    "# dbutils.widgets.text(\"Target_DB_Table\", \"\")\n",
    "# dbutils.widgets.text(\"Notification_Recipient\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5482732c-8607-4306-b6d5-a48ae57efb5d",
     "showTitle": true,
     "title": "Loading Configs"
    }
   },
   "outputs": [],
   "source": [
    "%run ./configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a143be-c24e-4403-b43c-77a61819cdaf",
     "showTitle": true,
     "title": "Validating Inputs"
    }
   },
   "outputs": [],
   "source": [
    "# Reading user inputs\n",
    "source_access = getArgument('Source_S3_Access').strip()\n",
    "source_s3_path = getArgument('Source_S3_File_Path').strip()\n",
    "target_db = getArgument('Target_DB')\n",
    "target_access = getArgument('Target_DB_Access').strip()\n",
    "target_table = getArgument('Target_DB_Table').strip()\n",
    "load_type = getArgument('Load_Type')\n",
    "header = getArgument('Header_Row')\n",
    "delimiter = getArgument('File_Delimiter').strip()\n",
    "notify_recipient = getArgument('Notification_Recipient').strip()\n",
    "s3_conn ={}\n",
    "db_conn ={}\n",
    "\n",
    "# Validating user inputs\n",
    "if source_access == '':\n",
    "  dbutils.notebook.exit(\"Source S3 Access name missing!\")\n",
    "elif source_s3_path == '':\n",
    "  dbutils.notebook.exit(\"Source S3 File Path missing!\")\n",
    "elif target_access == '':\n",
    "  dbutils.notebook.exit(\"Target DB Access Name missing!\")\n",
    "elif target_table == '':\n",
    "  dbutils.notebook.exit(\"Target Table missing!\")\n",
    "elif len(target_table.split('.')) != 2:\n",
    "  dbutils.notebook.exit(\"Schema name missing in target table!\")\n",
    "\n",
    "if source_access in s3_access_list: \n",
    "  s3_conn['s3_access_key'] = s3_access_keys[source_access]\n",
    "  s3_conn['s3_secret_key'] = s3_secret_keys[source_access]\n",
    "else:\n",
    "  dbutils.notebook.exit(\"Invalid S3 Access Name!\")\n",
    "\n",
    "if target_access in db_access_list:\n",
    "  db_conn['db_host'] = db_hosts[target_access]\n",
    "  db_conn['db_username'] = db_usernames[target_access]\n",
    "  db_conn['db_password'] = db_passwords[target_access]\n",
    "else:\n",
    "  dbutils.notebook.exit(\"Invalid DB Access Name!\")\n",
    "service = 'S3-DB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f88b399-a786-4537-9a8f-369e3fdfd0b6",
     "showTitle": true,
     "title": "Importing Required libraries"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import re\n",
    "import datetime, time\n",
    "import logging\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3418b4bf-7b7f-42a5-8fb4-5d1b9c319f12",
     "showTitle": true,
     "title": "Generating Run Identifier"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: be7a1a4a-b37c-46c9-9e16-64ae2d34cb50\n"
     ]
    }
   ],
   "source": [
    "run_id = uuid.uuid4()\n",
    "print('run_id:',run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "019b5458-a47f-4e2f-a4c1-901c860fbaf6",
     "showTitle": true,
     "title": "SQL Query exector function"
    }
   },
   "outputs": [],
   "source": [
    "def run_logger(service,log_op, opn, file_name, rc, status):\n",
    "  \"\"\"\n",
    "  Inputs: SQL Query\n",
    "  Output: Returns True if success\n",
    "  \"\"\"\n",
    "  logging.info(f\"Updating run log for {opn} operation....\")\n",
    "  if log_op == 'insert' :\n",
    "    query = f\"INSERT INTO TABLE run_log VALUES('{service}','{run_id}','{source_access}','{source_s3_path}','{file_name}','{opn}',{rc},'{target_access}','{target_table}','{status}',current_timestamp())\"\n",
    "  elif log_op == 'update':\n",
    "    query = f\"UPDATE run_log SET status='{status}' where run_id = '{run_id}' and operation='{opn}'\"\n",
    "  # Executing SQL Query\n",
    "  spark.sql(query)\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f3762c-91c6-47ca-93ff-74e1ea11dbd5",
     "showTitle": true,
     "title": "Function to check pattern and return latest file of that pattern"
    }
   },
   "outputs": [],
   "source": [
    "def file_pattern_check(source_s3_path,s3_conn):\n",
    "  \"\"\"\n",
    "  Inputs: S3 connection details, S3 file path\n",
    "  Output: Return file with max timestamp of pattern present in file path\n",
    "  \"\"\"\n",
    "  logging.info('Executing File pattern check......')\n",
    "\n",
    "  s3_parts_1 = source_s3_path.split('/')\n",
    "  bucket_name = s3_parts_1[2]\n",
    "  prefix = '/'.join(s3_parts_1[3:-1])\n",
    "  s3_file_part = s3_parts_1[-1]\n",
    "  s3_parts_2 = s3_file_part.split('_')\n",
    "  file_name_pattern = '_'.join(s3_parts_2[:-1])\n",
    "  file_format = source_s3_path.split('.')[-1]\n",
    "\n",
    "  # Accessing keys from connection inputs\n",
    "  access_key = s3_conn['s3_access_key']\n",
    "  secret_key = s3_conn['s3_secret_key']\n",
    "\n",
    "  # Creating boto3 client\n",
    "  s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "  \n",
    "  # listing required files from s3 location\n",
    "  obj_list = s3.list_objects(Bucket=bucket_name, Prefix=prefix)\n",
    "  objs = [item['Key'] for item in obj_list['Contents']]\n",
    "  obj_req = []\n",
    "  match_list = []\n",
    "  for obj in objs:\n",
    "    if obj.endswith(('.txt', '.csv', '.parquet')) and (obj.count('/') < 2):\n",
    "      obj = obj.split('/')[-1]\n",
    "      obj_req.append(obj)\n",
    "\n",
    "  # listing files with pattern match\n",
    "  if 'yyyymmddHHMMSS' in source_s3_path :\n",
    "    pattern = rf\"{file_name_pattern}_\\d{{14}}\\.{file_format}\"\n",
    "    for obj in obj_req :\n",
    "      if re.match(pattern, obj):\n",
    "        match_list.append(obj)\n",
    "  elif 'yyyymmdd' in source_s3_path :\n",
    "    pattern = rf\"{file_name_pattern}_\\d{{8}}\\.{file_format}\"\n",
    "    for obj in obj_req:\n",
    "      if re.match(pattern, obj):\n",
    "        match_list.append(obj)\n",
    "\n",
    "  if len(match_list) == 0:\n",
    "    logging.info(f\"No files found at {source_s3_path} for input file pattern\")\n",
    "    #run_logger('S3-DB','insert','read','','','failed')\n",
    "    return None\n",
    "  else:\n",
    "    # Selecting latest timestamp file for return\n",
    "    latest_file = max(match_list)\n",
    "    logging.info(f\"Latest file for pattern - {pattern} at s3://{bucket_name}/{prefix}: {latest_file}\")\n",
    "    return latest_file\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af31e348-2313-448f-bddb-d552e5b1261e",
     "showTitle": true,
     "title": "Function to read file from S3"
    }
   },
   "outputs": [],
   "source": [
    "def s3_file_read(s3_conn, source_s3_path) :\n",
    "\n",
    "  \"\"\"\n",
    "  Function to connect to user provided S3 access point\n",
    "  Read Files at S3 path\n",
    "  Create Spark df of file\n",
    "  Input: S3 connection keys, S3 File Path\n",
    "  Output: Spark DataFrame, file name\n",
    "  \"\"\"\n",
    "  logging.info('Executing S3 File read.......')\n",
    "\n",
    "  # Accessing keys from connection inputs\n",
    "  access_key = s3_conn['s3_access_key']\n",
    "  secret_key = s3_conn['s3_secret_key']\n",
    "\n",
    "  # Creating boto3 client\n",
    "  s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "  # Setting Spark configs to access S3\n",
    "  sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", access_key)\n",
    "  sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", secret_key)\n",
    "\n",
    "  # Extracting bucket name, prefix & file name from path\n",
    "  s3_parts = source_s3_path.split('/')\n",
    "  bucket_name = s3_parts[2] \n",
    "\n",
    "  if bucket_name == '':\n",
    "    logging.error(\"Invalid S3 File Path\")\n",
    "    run_logger('S3-DB','insert','read','','','failed')\n",
    "    return None\n",
    "  \n",
    "  prefix_1 = '/'.join(s3_parts[3:-1])\n",
    "  prefix_2 = '/'.join(s3_parts[3:])\n",
    "  header = getArgument(\"Header_Row\")\n",
    "  delimiter = getArgument(\"File_Delimiter\").strip()\n",
    "\n",
    "  # Check if file name is present in file path & create df accordingly\n",
    "  if source_s3_path.endswith(('.txt', '.csv', '.parquet')):\n",
    "    if ('yyyymmdd' in source_s3_path) or ('yyyymmddHHMMSS' in source_s3_path):\n",
    "      # Calling file pattern check function to get latest file\n",
    "      file_name = file_pattern_check(source_s3_path, s3_conn)\n",
    "      if file_name :\n",
    "        file_path = 's3://'+bucket_name+'/'+prefix_1+'/'+file_name\n",
    "        file_format = file_name.split('.')[1]\n",
    "        if file_format == 'parquet' :\n",
    "          df = spark.read.parquet(file_path)\n",
    "        else:\n",
    "          df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", header).option(\"delimiter\",delimiter).csv(file_path)\n",
    "      else:\n",
    "        logging.error(\"No latest file found\")\n",
    "        run_logger('S3-DB','insert','read','','','failed')\n",
    "        return None\n",
    "\n",
    "      rc = df.count()\n",
    "      # Calling run logger to insert record in log table\n",
    "      run_logger('S3-DB','insert','read',file_name,rc,'success')\n",
    "      return [df,file_name]\n",
    "        \n",
    "    else :\n",
    "      file_format = source_s3_path.split('.')[-1]\n",
    "      file_name = file_name = s3_parts[-1]\n",
    "      # Reading file into Spark DataFrame\n",
    "      if file_format == 'parquet':\n",
    "        df = spark.read.parquet(source_s3_path)\n",
    "      else:\n",
    "        df = spark.read.format(file_format).option(\"inferSchema\", \"true\").option(\"header\", header).option(\"delimiter\",delimiter).csv(source_s3_path)\n",
    "      rc = df.count()\n",
    "      # Calling run logger to insert record in log table\n",
    "      run_logger('S3-DB','insert','read',file_name,rc,'success')\n",
    "      return [df,file_name]\n",
    "\n",
    "  else:\n",
    "\n",
    "    file_name = ''\n",
    "    file_count = 0\n",
    "    # listing all files from s3 location\n",
    "    obj_list = s3.list_objects(Bucket=bucket_name, Prefix=prefix_2)\n",
    "    try:\n",
    "      objs = [item['Key'] for item in obj_list['Contents']]\n",
    "    except Exception as e:\n",
    "      logging.error(f\"Error during listing files at s3: {e}\")\n",
    "      run_logger('S3-DB','insert','read','',0,'failed')\n",
    "      return None\n",
    "    \n",
    "    for obj in objs:\n",
    "      if obj.endswith(('.txt', '.csv', '.parquet')) and (obj.count('/')<2):\n",
    "        file_format = obj.split('.')[-1]\n",
    "        file_count+=1\n",
    "        file = obj.split('/')[-1]\n",
    "        file_name = file_name + file + '|'\n",
    "    file_name = file_name.strip('|')\n",
    "\n",
    "    if file_name:\n",
    "      if file_format == 'parquet':\n",
    "        df = spark.read.parquet(source_s3_path)\n",
    "        file_name =''\n",
    "      else:\n",
    "        df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", header).option(\"delimiter\",delimiter).csv(source_s3_path)\n",
    "      rc= df.count()\n",
    "     \n",
    "      logging.info(\"Completed reading files to df ........\")\n",
    "      run_logger('S3-DB','insert','read','MultipleFiles',rc,'success')\n",
    "      return [df,file_name]\n",
    "      \n",
    "    else:\n",
    "      logging.info(\"No files found at source s3 ......\")\n",
    "      run_logger('S3-DB','insert','read','',0,'failed')\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88f794c-71a8-4cfe-9aff-60b551b847e8",
     "showTitle": true,
     "title": "Function to write file at Target table in Database"
    }
   },
   "outputs": [],
   "source": [
    "def db_file_write(db_conn, target_table, input_df):\n",
    "\n",
    "  \"\"\"\n",
    "  Receives Spark DataFrame from reader function\n",
    "  Writes df to target DB table\n",
    "  Input: Target access connection details, Target Database table name, Spark df from reader function\n",
    "  Output: True False status of write, record count\n",
    "  \"\"\"\n",
    "  logging.info(\"Executing file writing function .....\")\n",
    "\n",
    "  db_name = target_table.split('.')[0]\n",
    "  dbtable = target_table.split('.')[1]\n",
    "  \n",
    "  # Input record count\n",
    "  rc = input_df.count()\n",
    "\n",
    "  # Preparing url string for JDBC connection\n",
    "  if target_db == 'MySQL':\n",
    "    url = f\"jdbc:mysql://{db_conn['db_host']}:3306/{db_name}\" \n",
    "  elif target_db == 'PSQL':\n",
    "    url = f\"jdbc:postgresql://{db_conn['db_host']}:5432/{db_name}\" \n",
    "\n",
    "  # Writing input df to target table\n",
    "  (\n",
    "  input_df.write\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", url)\n",
    "  .option(\"dbtable\",dbtable)\n",
    "  .option(\"user\", db_conn['db_username'])\n",
    "  .option(\"password\", db_conn['db_password'])\n",
    "  .mode(load_type)\n",
    "  .save()\n",
    "  )\n",
    "  logging.info(\"Completed writing to table.......\")\n",
    "  return rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0243ec3c-c536-4c38-8e27-dcf062056127",
     "showTitle": true,
     "title": "Main Function"
    }
   },
   "outputs": [],
   "source": [
    "def s3_db(s3_conn, source_s3_path, db_conn, target_table):\n",
    "  \n",
    "  \"\"\"\n",
    "  Main function to call read and write functions\n",
    "  Inputs: source parameters, target prameters\n",
    "  Output: record count\n",
    "  \"\"\"\n",
    "  inputs = s3_file_read(s3_conn, source_s3_path)\n",
    "  if inputs:\n",
    "    result = db_file_write(db_conn, target_table, inputs[0])\n",
    "  else:\n",
    "    #run_logger('S3-DB','update','read','','','failed')\n",
    "    result = None\n",
    "\n",
    "  if result:\n",
    "    run_logger('S3-DB','insert','write',f'{inputs[1]}',f'{result}','success')\n",
    "  else:\n",
    "    run_logger('S3-DB','insert','write','',0,'failed')\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2721a8f9-3b1a-4ad4-8c00-bd1f80f41ed8",
     "showTitle": true,
     "title": "Initiating main function call"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Executing S3 File read.......\nINFO:root:Executing File pattern check......\nINFO:root:Latest file for pattern - sftp_\\d{8}\\.txt at s3://sdevalla-portfolio/orch_test: sftp_20240323.txt\nINFO:root:Updating run log for read operation....\nINFO:root:Executing file writing function .....\nINFO:root:Completed writing to table.......\nINFO:root:Updating run log for write operation....\nINFO:root:918 records transferred from s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt to reddit.reddit_posts_agg_c\n"
     ]
    }
   ],
   "source": [
    "# Calling main function\n",
    "status = s3_db(s3_conn, source_s3_path, db_conn, target_table)\n",
    "\n",
    "# Checking status\n",
    "if status :\n",
    "  logging.info(f\"{status} records transferred from {source_s3_path} to {target_table}\")\n",
    "else:\n",
    "  logging.info(\"Failed to transfer file from S3 to Target table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ec6726-42e2-482e-b25f-d193a9fa1d84",
     "showTitle": true,
     "title": "Function to send Email notification"
    }
   },
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "def send_email(subject, body_html, sender, recipients):\n",
    "\n",
    "  \"\"\"\n",
    "  This function sends email notification to recipeints on run status\n",
    "  Inputs: Email content, receiver email address list\n",
    "  Output: Success/Failure message\n",
    "  \"\"\"\n",
    "  access_key = ses_conn['access_key']\n",
    "  secret_key = ses_conn['secret_key']\n",
    "\n",
    "  #Creating boto3 ses client\n",
    "  ses_client = boto3.client('ses', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name = 'us-east-2')\n",
    "\n",
    "  # Create a MIME message\n",
    "  body_text = \"This email requires HTML support. Please view in a HTML-compatible email client.\"\n",
    "  charset = \"UTF-8\"\n",
    "  \n",
    "  # Assemble the email\n",
    "  try:\n",
    "      response = ses_client.send_email(\n",
    "          Destination={\n",
    "              'ToAddresses': recipients,\n",
    "          },\n",
    "          Message={\n",
    "              'Body': {\n",
    "                  'Html': {\n",
    "                      'Charset': charset,\n",
    "                      'Data': body_html,\n",
    "                  },\n",
    "                  'Text': {\n",
    "                      'Charset': charset,\n",
    "                      'Data': body_text,\n",
    "                  },\n",
    "              },\n",
    "              'Subject': {\n",
    "                  'Charset': charset,\n",
    "                  'Data': subject,\n",
    "              },\n",
    "          },\n",
    "          Source=sender,\n",
    "      )\n",
    "  except ClientError as e:\n",
    "      print(e.response['Error']['Message'])\n",
    "  else:\n",
    "      print(\"Email sent! Message ID:\", response['MessageId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698ee1da-ad65-4704-bc58-6854d55c15be",
     "showTitle": true,
     "title": "Function to create HTML Body"
    }
   },
   "outputs": [],
   "source": [
    "def dataframe_to_html_table(df):\n",
    "    # Convert DataFrame to HTML table\n",
    "    html_table = df.to_html(index=False)\n",
    "    # Add inline CSS styling to color the header\n",
    "    styled_header = '<th style=\"background-color: #FB451D; color: white;\">'\n",
    "    html_table = html_table.replace('<th>', styled_header)\n",
    "    # Format the HTML table\n",
    "    formatted_html_table = f'<html><body>{html_table}</body></html>'\n",
    "    return formatted_html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188589f9-a5d7-4bb9-a57e-8b54c4d3b25b",
     "showTitle": true,
     "title": "Calling Email send function"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><p>Hi,</p><p>Please find the status of service: S3-DB with run id: be7a1a4a-b37c-46c9-9e16-64ae2d34cb50</p><html><body><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Service</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Source_S3</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Source File</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Task</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Record_Count</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Target_Table</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Status</th>\n",
       "      <th style=\"background-color: #FB451D; color: white;\">Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>S3-DB</td>\n",
       "      <td>s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt</td>\n",
       "      <td>sftp_20240323.txt</td>\n",
       "      <td>read</td>\n",
       "      <td>918</td>\n",
       "      <td>reddit.reddit_posts_agg_c</td>\n",
       "      <td>success</td>\n",
       "      <td>2024-03-23 16:25:11.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>S3-DB</td>\n",
       "      <td>s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt</td>\n",
       "      <td>sftp_20240323.txt</td>\n",
       "      <td>write</td>\n",
       "      <td>918</td>\n",
       "      <td>reddit.reddit_posts_agg_c</td>\n",
       "      <td>success</td>\n",
       "      <td>2024-03-23 16:26:03.274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></body></html></body></html>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<html><body><p>Hi,</p><p>Please find the status of service: S3-DB with run id: be7a1a4a-b37c-46c9-9e16-64ae2d34cb50</p><html><body><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th style=\"background-color: #FB451D; color: white;\">Service</th>\n      <th style=\"background-color: #FB451D; color: white;\">Source_S3</th>\n      <th style=\"background-color: #FB451D; color: white;\">Source File</th>\n      <th style=\"background-color: #FB451D; color: white;\">Task</th>\n      <th style=\"background-color: #FB451D; color: white;\">Record_Count</th>\n      <th style=\"background-color: #FB451D; color: white;\">Target_Table</th>\n      <th style=\"background-color: #FB451D; color: white;\">Status</th>\n      <th style=\"background-color: #FB451D; color: white;\">Timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>S3-DB</td>\n      <td>s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt</td>\n      <td>sftp_20240323.txt</td>\n      <td>read</td>\n      <td>918</td>\n      <td>reddit.reddit_posts_agg_c</td>\n      <td>success</td>\n      <td>2024-03-23 16:25:11.939</td>\n    </tr>\n    <tr>\n      <td>S3-DB</td>\n      <td>s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt</td>\n      <td>sftp_20240323.txt</td>\n      <td>write</td>\n      <td>918</td>\n      <td>reddit.reddit_posts_agg_c</td>\n      <td>success</td>\n      <td>2024-03-23 16:26:03.274</td>\n    </tr>\n  </tbody>\n</table></body></html></body></html>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No recipients to send email!\n"
     ]
    }
   ],
   "source": [
    "subject = f\"{service} run status for run id - {run_id}\"\n",
    "df = spark.sql(\"\"\" select distinct service as Service, source_path_table as `Source_S3`, source_file_dml as `Source File`, operation as Task,  record_count as Record_Count, target_path_table as Target_Table, Status, Timestamp from run_log where run_id = '{}' order by task \"\"\".format(run_id ))\n",
    "html_table = dataframe_to_html_table(df.toPandas())\n",
    "body_html = f\"<html><body><p>Hi,</p><p>Please find the status of service: {service} with run id: {run_id}</p>{html_table}</body></html>\"\n",
    "displayHTML(body_html)\n",
    "sender = \"noreplyd22snotification@gmail.com\"\n",
    "if notify_recipient != '' :\n",
    "  recipients = notify_recipient.split(',')\n",
    "else:\n",
    "   recipients = []\n",
    "if len(recipients) > 0:\n",
    "  send_email(subject, body_html, sender, recipients)\n",
    "else:\n",
    "  print('No recipients to send email!')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2105701564358612,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "Source_S3_Access",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Source_S3_File_Path",
      "width": 711
     },
     {
      "breakBefore": false,
      "name": "Header_Row",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "File_Delimiter",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Load_Type",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Target_DB",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Target_DB_Access",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Target_DB_Table",
      "width": 529
     },
     {
      "breakBefore": false,
      "name": "Notification_Recipient",
      "width": 529
     }
    ]
   },
   "notebookName": "S3-DB",
   "widgets": {
    "File_Delimiter": {
     "currentValue": "\\t",
     "nuid": "63b7ce6f-8e84-4981-9d48-9d8f61f37615",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": ",",
      "label": null,
      "name": "File_Delimiter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Header_Row": {
     "currentValue": "True",
     "nuid": "084eb4d4-b0a2-4663-bd10-f98e343b0de2",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "True",
      "label": null,
      "name": "Header_Row",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "Load_Type": {
     "currentValue": "Append",
     "nuid": "a7bfd7dd-3779-467c-bfdd-3f3d920f671a",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Append",
      "label": null,
      "name": "Load_Type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Append",
        "Overwrite"
       ]
      }
     }
    },
    "Notification_Recipient": {
     "currentValue": "sdevalla@purdue.edu",
     "nuid": "a09747e9-49d9-4ea0-b24b-5e409ab98c8b",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Notification_Recipient",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Source_S3_Access": {
     "currentValue": "sd_s3",
     "nuid": "c2a39fb4-6899-4ec2-8d38-73aa2b67f8c0",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_S3_Access",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Source_S3_File_Path": {
     "currentValue": "s3://sdevalla-portfolio/orch_test/sftp_yyyymmdd.txt",
     "nuid": "f7c530c6-be35-45c6-9763-f7bc31ac0245",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Source_S3_File_Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_DB": {
     "currentValue": "MySQL",
     "nuid": "d3b665e8-e67c-4b6e-8053-6545b1dde724",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "MySQL",
      "label": null,
      "name": "Target_DB",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "MySQL",
        "PSQL"
       ]
      }
     }
    },
    "Target_DB_Access": {
     "currentValue": "sd_rds",
     "nuid": "04660aca-514d-4f5d-acb7-28d0c8eecf7c",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_DB_Access",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Target_DB_Table": {
     "currentValue": "reddit.reddit_posts_agg_c",
     "nuid": "d17d1826-02bf-4ffc-936b-fed7ceb3b27c",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Target_DB_Table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
